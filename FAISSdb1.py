from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
import os
import re

# ì €ì¥í•  ë²•ë¥  íŒŒì¼ ëª©ë¡ (ê²½ë¡œ ì„¤ì • ìœ ì§€)
law_files = {
    "Police_Duties_Act": "pdf/txt/ê²½ì°°ê´€ì§ë¬´ì§‘í–‰ë²•.txt",
    "Labor_Standards_Act": "pdf/txt/ê·¼ë¡œê¸°ì¤€ë²•.txt",
    "Road_Traffic_Act": "pdf/txt/ë„ë¡œêµí†µë²•.txt",
    "Civil_Law": "pdf/txt/ë¯¼ë²•.txt",
    "Criminal_Law": "pdf/txt/í˜•ë²•.txt"
}

# FAISS ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •
FAISS_INDEX_DIR = "faiss_indexes"
os.makedirs(FAISS_INDEX_DIR, exist_ok=True)

# SBERT ì„ë² ë”© ëª¨ë¸ (768ì°¨ì›)
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli", model_kwargs={"device": "cpu"})

# ê¸°ì¡´ ë²¡í„°DB ë®ì–´ì“°ê¸° ì—¬ë¶€ ì„¤ì •
overwrite = True  

# ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë²• ì¡°í•­ ë‹¨ìœ„ë¡œ ë¶„í• 
def split_law_articles(text):
    pattern = r"(ì œ\d+ì¡°\([^)]+\))"  # "ì œ123ì¡°(ì œëª©)" íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    matches = re.split(pattern, text)

    articles = []
    for i in range(1, len(matches), 2):  # ë²ˆí˜¸ì™€ ë³¸ë¬¸ì„ í•©ì³ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        article_title = matches[i]
        article_body = matches[i+1] if i+1 < len(matches) else ""
        articles.append(article_title + article_body.strip())

    return articles

# ëª¨ë“  ë²•ë¥ ì— ëŒ€í•´ FAISS ë²¡í„°DB ìƒì„±
for law_name, file_path in law_files.items():
    index_path = os.path.join(FAISS_INDEX_DIR, law_name)

    # ê¸°ì¡´ ë²¡í„°DBê°€ ìˆê³ , ë®ì–´ì“°ì§€ ì•ŠëŠ”ë‹¤ë©´ ìŠ¤í‚µ
    if not overwrite and os.path.exists(index_path):
        print(f"âš ï¸ {law_name} ë²¡í„°DBê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤...")
        continue

    print(f"ğŸ“– {law_name} ë²¡í„°DBë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...")

    # ë²•ë¥  íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(file_path):
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {file_path}")
        continue

    with open(file_path, "r", encoding="utf-8") as f:
        text_content = f.read()

    # ë²• ì¡°í•­ ë‹¨ìœ„ë¡œ ë¶„í• 
    text_chunks = split_law_articles(text_content)

    if not text_chunks:
        print(f"âš ï¸ {law_name}ì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤...")
        continue

    # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì¶œì²˜ ì •ë³´ + ë²• ì¡°í•­ ë²ˆí˜¸)
    documents = []
    for chunk in text_chunks:
        article_num = re.search(r"ì œ\d+ì¡°", chunk)  # ì¡°í•­ ë²ˆí˜¸ ì¶”ì¶œ
        article_title = article_num.group() if article_num else "Unknown"
        documents.append(Document(
            page_content=chunk,
            metadata={"source": law_name, "article": article_title}  # ë²•ëª… + ì¡°í•­ ë²ˆí˜¸ ì¶”ê°€
        ))

    # FAISS ë²¡í„°DB ìƒì„± (`from_documents` ì‚¬ìš© + L2 ì •ê·œí™” ì ìš©)
    vector_store = FAISS.from_documents(documents, embedding_model, normalize_L2=True)

    # ê°œë³„ í´ë”ì— ì €ì¥
    vector_store.save_local(index_path)

    print(f"âœ… {law_name} ë²¡í„°DBê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! ({len(text_chunks)}ê°œì˜ ì¡°í•­ì´ ì²˜ë¦¬ë¨)")

print("ğŸ‰ ëª¨ë“  ë²•ë¥  ë°ì´í„°ê°€ FAISS ë²¡í„°DBì— ì €ì¥ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€")
